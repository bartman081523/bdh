Repository Documentation
This document provides a comprehensive overview of the repository's structure and contents.
The first section, titled 'Directory/File Tree', displays the repository's hierarchy in a tree format.
In this section, directories and files are listed using tree branches to indicate their structure and relationships.
Following the tree representation, the 'File Content' section details the contents of each file in the repository.
Each file's content is introduced with a '[File Begins]' marker followed by the file's relative path,
and the content is displayed verbatim. The end of each file's content is marked with a '[File Ends]' marker.
This format ensures a clear and orderly presentation of both the structure and the detailed contents of the repository.

Directory/File Tree Begins -->

/
├── bdh_neuro
│   ├── bdh_gpu_neuro_ref.py
│   ├── bdh_gpu_neuro_torch.py
│   ├── bdh_gpu_ref_stabilized.py
│   ├── data
│   ├── example.py
│   ├── example_cuda.py
│   ├── generate_bdh_baseline.py
│   ├── generate_bdh_neuro.py
│   ├── plot_sparsity.py
│   ├── train_bdh_baseline.py
│   ├── train_bdh_neuro.py
│   └── train_bdh_refstyle.py
├── bdh_original
│   ├── LICENSE.md
│   ├── README.md
│   ├── bdh.py
│   ├── figs
│   └── train.py

<-- Directory/File Tree Ends

File Content Begin -->
[File Begins] bdh_neuro/bdh_gpu_neuro_ref.py

from __future__ import annotations
import numpy as np
from dataclasses import dataclass, field
from typing import Optional, List, Dict

Array = np.ndarray

def relu(z: Array) -> Array:
    return np.maximum(0.0, z)

def layernorm_row(v: Array, eps: float = 1e-6) -> Array:
    m = v.mean(axis=-1, keepdims=True)
    s = v.std(axis=-1, keepdims=True)
    return (v - m) / (s + eps)

def softmax(z: Array, axis: int = -1) -> Array:
    z = z - np.max(z, axis=axis, keepdims=True)
    return np.exp(z) / np.sum(np.exp(z), axis=axis, keepdims=True)

def effective_rank(mat: Array, eps: float = 1e-12) -> float:
    s = np.linalg.svd(mat, compute_uv=False)
    ps = (s**2) / (np.sum(s**2) + eps)
    H = -np.sum(ps * (np.log(ps + eps)))
    return float(np.exp(H))

# ----------------------------- Baseline (Definition 4) -----------------------------

@dataclass
class BDHGPURef:
    """
    Minimal, faithful implementation of BDH-GPU (Definition 4) in NumPy.

    States:
        x_t in R^n_{>=0}, y_t in R^n_{>=0}, v*_t in R^d, rho_t in R^{d x n}

    Update (per token):
        x_t = x_{t-1} + (Dx v*_{t-1})_+                          [ReLU-lowrank path]
        a*_t = rho_{t-1} x_t
        y_t = (Dy LN(a*_t))_+ ⊙ x_t                               [LN in d-space]
        v*_t = LN(E y_t)
        rho_t = (rho_{t-1} + v*_{t-1} x_t^T) * U                  [rank-1 increment]

    This mirrors the paper's Definition 4 up to the placement of LN, which can be toggled.
    """
    n: int = 256
    d: int = 32
    V: int = 4096
    seed: int = 0
    u_decay: float = 0.97
    ln_before_Dy: bool = True
    use_relu_lowrank: bool = True

    E: Array = field(init=False)
    Dx: Array = field(init=False)
    Dy: Array = field(init=False)
    token_emb: Array = field(init=False)

    x: Array = field(init=False)
    y: Array = field(init=False)
    v: Array = field(init=False)
    rho: Array = field(init=False)

    rng: np.random.Generator = field(init=False)

    def __post_init__(self):
        self.rng = np.random.default_rng(self.seed)
        self.E = self.rng.standard_normal((self.d, self.n)) * (1.0/np.sqrt(self.n))
        self.Dx = self.rng.standard_normal((self.n, self.d)) * (1.0/np.sqrt(self.d))
        self.Dy = self.rng.standard_normal((self.n, self.d)) * (1.0/np.sqrt(self.d))
        self.token_emb = self.rng.standard_normal((self.V, self.d)) * (1.0/np.sqrt(self.d))

        self.x = np.zeros((self.n,), dtype=float)
        self.y = np.zeros((self.n,), dtype=float)
        self.v = np.zeros((self.d,), dtype=float)
        self.rho = np.zeros((self.d, self.n), dtype=float)

    def _LN_d(self, z: Array) -> Array:
        return layernorm_row(z.reshape(1, -1)).ravel()

    def step(self, token_index: int) -> Dict[str, float]:
        v_prev = self.token_emb[token_index]
        x_in = self.Dx @ v_prev
        # >>> Uses ReLU-lowrank as in the paper.
        x_t = self.x + (relu(x_in) if self.use_relu_lowrank else x_in)

        a_star = self.rho @ x_t

        if self.ln_before_Dy:
            y_core = self.Dy @ self._LN_d(a_star)
        else:
            y_core = self._LN_d(self.Dy @ a_star)
        y_t = relu(y_core) * np.maximum(0.0, x_t)

        v_star = self._LN_d(self.E @ y_t)

        # >>> Rank-1 outer product increment on rho, then scalar decay U (baseline).
        self.rho = self.u_decay * (self.rho + v_prev.reshape(self.d,1) @ x_t.reshape(1,self.n))

        self.x, self.y, self.v = x_t, y_t, v_star

        spars_x = 1.0 - (np.count_nonzero(self.x) / self.n)
        spars_y = 1.0 - (np.count_nonzero(self.y) / self.n)
        return dict(sparsity_x=spars_x, sparsity_y=spars_y,
                    rho_F=float(np.linalg.norm(self.rho, ord='fro')),
                    rho_eff_rank=effective_rank(self.rho))

    def run(self, T: int) -> Dict[str, float]:
        out = {}
        for _ in range(T):
            idx = int(self.rng.integers(0, self.V))
            out = self.step(idx)
        return out

# ----------------------------- Neuro‑inspired extension -----------------------------

@dataclass
class BDHNeuroRef(BDHGPURef):
    """
    Drop-in replacement that adds neuro-inspired mechanisms.
    Each block below is annotated with:
        >>> DIFFERENCE vs BDH (Def.4): <what changed>
    """
    # (1) Multi-timescale STDP-like kernels for U (mixture of decays)
    U_kernels: Optional[List[float]] = None
    U_weights: Optional[List[float]] = None

    # (2) Local, activity-dependent forgetting in rho
    local_forget_eta: float = 0.0

    # (3) Homeostatic target activity for y
    homeostasis_tau: Optional[float] = None
    homeostasis_beta: float = 0.05

    # (4) k-WTA / lateral inhibition in n-space
    k_wta: Optional[int] = None

    # (5) Dendritic subunits (branch nonlinearity across row-splits of Dy)
    branches: int = 0
    branch_nl: str = "softplus"  # "softplus" or "relu"

    # (6) Neuromodulation via surprisal (entropy of v*)
    mod_gamma_max: float = 1.0

    # (7) Stochastic spiking (Bernoulli noise before ReLU)
    spike_rate: float = 0.0

    def _branch_nonlin(self, z: Array) -> Array:
        if self.branch_nl == "softplus":
            return np.log1p(np.exp(z))
        return np.maximum(0.0, z)

    def _surprisal_gain(self, v_star: Array) -> float:
        if self.mod_gamma_max <= 0.0:
            return 1.0
        p = softmax(v_star.reshape(1,-1), axis=-1).ravel()
        H = -np.sum(p * np.log(p + 1e-12))
        return float(min(self.mod_gamma_max, H / np.log(self.d + 1e-6) * self.mod_gamma_max))

    def step(self, token_index: int) -> Dict[str, float]:
        v_prev = self.token_emb[token_index]

        # Same x-path as baseline (optional ReLU-lowrank).
        x_in = self.Dx @ v_prev
        x_t = self.x + (relu(x_in) if self.use_relu_lowrank else x_in)

        # Linear attention readout.
        a_star = self.rho @ x_t

        # >>> DIFFERENCE vs BDH (Def.4): dendritic subunits across Dy rows.
        if self.branches > 0:
            a_hat = layernorm_row(a_star.reshape(1,-1)).ravel() if self.ln_before_Dy else a_star
            splits = np.array_split(self.Dy, self.branches, axis=0)
            parts = [self._branch_nonlin(Dy_b @ a_hat) for Dy_b in splits]
            y_core = np.concatenate(parts, axis=0)  # back to length n
            if not self.ln_before_Dy:
                y_core = layernorm_row(y_core.reshape(1,-1)).ravel()
        else:
            if self.ln_before_Dy:
                y_core = self.Dy @ layernorm_row(a_star.reshape(1,-1)).ravel()
            else:
                y_core = layernorm_row((self.Dy @ a_star).reshape(1,-1)).ravel()

        # >>> DIFFERENCE: stochastic spikes before thresholding.
        if self.spike_rate > 0.0:
            spikes = (np.random.random(size=y_core.shape) < self.spike_rate).astype(y_core.dtype)
            y_core = y_core + spikes

        # >>> DIFFERENCE: k-WTA lateral inhibition (n-space).
        if self.k_wta is not None and 0 < self.k_wta < self.n:
            idx = np.argpartition(y_core, -self.k_wta)[-self.k_wta:]
            mask = np.zeros_like(y_core, dtype=bool)
            mask[idx] = True
            y_core = y_core * mask

        # Nonnegativity & gating by x≥0 (as in baseline).
        y_t = relu(y_core) * np.maximum(0.0, x_t)

        # >>> DIFFERENCE: homeostatic scaling towards target L1(y).
        if self.homeostasis_tau is not None:
            s = float(np.sum(y_t))
            if s > 1e-8:
                scale = min(1.0, self.homeostasis_tau / (s + 1e-8))
                y_t = y_t * scale

        v_star = layernorm_row((self.E @ y_t).reshape(1,-1)).ravel()

        # >>> DIFFERENCE: local forgetting for inactive presynapses on rho.
        rho_next = self.rho.copy()
        if self.local_forget_eta > 0.0:
            inactive = (x_t <= 0.0).astype(rho_next.dtype)
            rho_next = rho_next * (1.0 - self.local_forget_eta * inactive.reshape(1, self.n))

        # >>> DIFFERENCE: neuromodulated, multi-timescale rho update.
        gain = self._surprisal_gain(v_star)  # ∈ (0, mod_gamma_max]
        inc = gain * (v_prev.reshape(self.d,1) @ x_t.reshape(1,self.n))  # rank-1

        if self.U_kernels is None:
            rho_next = self.u_decay * (rho_next + inc)  # baseline scalar U
        else:
            wk = np.array(self.U_weights if self.U_weights is not None else [1.0]*len(self.U_kernels), dtype=float)
            wk = wk / np.sum(wk)
            rho_mix = np.zeros_like(rho_next)
            for w, u in zip(wk, self.U_kernels):
                rho_mix += w * (u * (rho_next + inc))
            rho_next = rho_mix

        self.rho = rho_next
        self.x, self.y, self.v = x_t, y_t, v_star

        spars_x = 1.0 - (np.count_nonzero(self.x) / self.n)
        spars_y = 1.0 - (np.count_nonzero(self.y) / self.n)
        return dict(sparsity_x=spars_x, sparsity_y=spars_y,
                    rho_F=float(np.linalg.norm(self.rho, ord='fro')),
                    rho_eff_rank=effective_rank(self.rho), gain=gain)

if __name__ == "__main__":
    # Tiny smoke test
    base = BDHGPURef(n=64, d=16, V=512, seed=3)
    print("baseline:", base.run(T=8))
    neuro = BDHNeuroRef(n=64, d=16, V=512, seed=3,
                        U_kernels=[0.99,0.97,0.94], U_weights=[0.5,0.3,0.2],
                        local_forget_eta=0.02, homeostasis_tau=0.15*64,
                        k_wta=8, branches=2, branch_nl="softplus",
                        mod_gamma_max=0.8, spike_rate=0.01)
    print("neuro:", neuro.run(T=8))

[File Ends] bdh_neuro/bdh_gpu_neuro_ref.py

[File Begins] bdh_neuro/bdh_gpu_neuro_torch.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BDH-Neuro Reference Implementation (PyTorch)
--------------------------------------------
Neuro-inspired variant of BDH-GPU (Def.4) with biological dynamics.

✔ Multi-timescale synaptic decay (STDP-like)
✔ Local forgetting for inactive pre-synapses
✔ Homeostatic regulation of output activity
✔ k-WTA lateral inhibition on y (and adaptive k-WTA on x)
✔ Dendritic subunits with branch nonlinearities
✔ Neuromodulated rho updates via surprisal gain
✔ Stochastic spikes
✔ x-decay + normalization for realistic sparsity
✔ NEW: adaptive x-sparsity (gain-modulated Option C)
"""

import torch
from typing import Optional, List, Dict


def relu(z):
    return torch.clamp_min(z, 0.0)


def layernorm_row(z, eps: float = 1e-6):
    m = z.mean(dim=-1, keepdim=True)
    s = z.std(dim=-1, keepdim=True)
    return (z - m) / (s + eps)


def effective_rank(mat: torch.Tensor, eps: float = 1e-12) -> float:
    with torch.no_grad():
        s = torch.linalg.svdvals(mat)
        ps = (s**2) / (s.pow(2).sum() + eps)
        H = -(ps * (ps.add(eps).log())).sum()
        return float(torch.exp(H))


class BDHGPURefTorch(torch.nn.Module):
    """Baseline BDH-GPU (Definition 4)."""
    def __init__(self, n=256, d=32, V=4096, seed=0, u_decay=0.97,
                 ln_before_Dy=True, use_relu_lowrank=True, device="cpu"):
        super().__init__()
        g = torch.Generator(device=device).manual_seed(seed)
        self.n, self.d, self.V = n, d, V
        self.u_decay = float(u_decay)
        self.ln_before_Dy = bool(ln_before_Dy)
        self.use_relu_lowrank = bool(use_relu_lowrank)
        self.device = device

        self.E  = torch.randn(d, n, generator=g, device=device) / (n**0.5)
        self.Dx = torch.randn(n, d, generator=g, device=device) / (d**0.5)
        self.Dy = torch.randn(n, d, generator=g, device=device) / (d**0.5)
        self.token_emb = torch.randn(V, d, generator=g, device=device) / (d**0.5)

        self.register_buffer("x", torch.zeros(n, device=device))
        self.register_buffer("y", torch.zeros(n, device=device))
        self.register_buffer("v", torch.zeros(d, device=device))
        self.register_buffer("rho", torch.zeros(d, n, device=device))
        self._rng = g

    def step(self, token_index: int) -> Dict[str, float]:
        v_prev = self.token_emb[int(token_index)]
        x_t = self.x + (relu(self.Dx @ v_prev) if self.use_relu_lowrank else (self.Dx @ v_prev))
        a_star = self.rho @ x_t
        if self.ln_before_Dy:
            y_core = self.Dy @ layernorm_row(a_star)
        else:
            y_core = layernorm_row(self.Dy @ a_star)
        y_t = relu(y_core) * torch.clamp_min(x_t, 0.0)
        v_star = layernorm_row(self.E @ y_t)
        self.rho = self.u_decay * (self.rho + v_prev.view(self.d,1) @ x_t.view(1,self.n))
        self.x, self.y, self.v = x_t, y_t, v_star

        with torch.no_grad():
            spars_x = 1.0 - float((self.x != 0).float().mean().item())
            spars_y = 1.0 - float((self.y != 0).float().mean().item())
            rho_F = float(torch.linalg.norm(self.rho).item())
            rho_er = effective_rank(self.rho)
        return dict(sparsity_x=spars_x, sparsity_y=spars_y,
                    rho_F=rho_F, rho_eff_rank=rho_er)

    @torch.no_grad()
    def run(self, T: int) -> Dict[str, float]:
        out = {}
        for _ in range(T):
            idx = torch.randint(0, self.V, (1,), generator=self._rng, device=self.device).item()
            out = self.step(idx)
        return out


class BDHNeuroRefTorch(BDHGPURefTorch):
    """Neuro-inspired BDH variant with biologically plausible extensions."""
    def __init__(self, *args,
                 U_kernels: Optional[List[float]] = None, U_weights: Optional[List[float]] = None,
                 local_forget_eta: float = 0.0, homeostasis_tau: Optional[float] = None,
                 k_wta: Optional[int] = None, branches: int = 0, branch_nl: str = "softplus",
                 mod_gamma_max: float = 1.0, spike_rate: float = 0.0,
                 x_decay: float = 0.97,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.U_kernels = U_kernels
        self.U_weights = U_weights
        self.local_forget_eta = float(local_forget_eta)
        self.homeostasis_tau = homeostasis_tau
        self.k_wta = k_wta
        self.branches = int(branches)
        self.branch_nl = branch_nl
        self.mod_gamma_max = float(mod_gamma_max)
        self.spike_rate = float(spike_rate)
        self.x_decay = x_decay

    def _branch_nl(self, z):
        if self.branch_nl == "softplus":
            return torch.nn.functional.softplus(z)
        return relu(z)

    def _surprisal_gain(self, v_star: torch.Tensor) -> float:
        if self.mod_gamma_max <= 0.0:
            return 1.0
        p = torch.softmax(v_star, dim=-1)
        H = -(p * (p + 1e-12).log()).sum()
        return float(min(self.mod_gamma_max,
                         (H / torch.log(torch.tensor(float(self.d) + 1e-6))).item()
                         * self.mod_gamma_max))

    def step(self, token_index: int) -> Dict[str, float]:
        v_prev = self.token_emb[int(token_index)]

        # --- x dynamics: decay + normalization
        x_t = self.x_decay * self.x + (relu(self.Dx @ v_prev)
                                       if self.use_relu_lowrank
                                       else (self.Dx @ v_prev))
        x_t = x_t / (x_t.norm(p=1) + 1e-6)

        # temporary gain placeholder (computed later)
        gain = 0.7

        # --- core drive before computing gain
        a_star = self.rho @ x_t

        # Dendritic subunits
        if self.branches and self.branches > 0:
            a_hat = layernorm_row(a_star) if self.ln_before_Dy else a_star
            splits = torch.tensor_split(self.Dy, self.branches, dim=0)
            parts = [self._branch_nl(Dy_b @ a_hat) for Dy_b in splits]
            y_core = torch.cat(parts, dim=0)
            if not self.ln_before_Dy:
                y_core = layernorm_row(y_core)
        else:
            if self.ln_before_Dy:
                y_core = self.Dy @ layernorm_row(a_star)
            else:
                y_core = layernorm_row(self.Dy @ a_star)

        # Spikes
        if self.spike_rate > 0.0:
            u = torch.rand_like(y_core)
            y_core = y_core + (u < self.spike_rate).float()

        # Lateral inhibition on y
        if self.k_wta is not None and 0 < self.k_wta < self.n:
            vals, idx = torch.topk(y_core, self.k_wta)
            mask = torch.zeros_like(y_core, dtype=torch.bool)
            mask[idx] = True
            y_core = y_core * mask.float()

        y_t = relu(y_core) * torch.clamp_min(x_t, 0.0)

        # Homeostasis
        if self.homeostasis_tau is not None:
            s = y_t.sum().item()
            if s > 1e-8:
                scale = min(1.0, self.homeostasis_tau / (s + 1e-8))
                y_t = y_t * scale

        v_star = layernorm_row(self.E @ y_t)

        # --- recompute gain now that v_star is known
        gain = self._surprisal_gain(v_star)

        # --- adaptive k-WTA on x (Option C: gain-modulated)
        k_dynamic = int(self.n * (0.05 + 0.25 * gain))
        k_dynamic = max(1, min(k_dynamic, self.n))
        vals, idx = torch.topk(x_t, k_dynamic)
        mask = torch.zeros_like(x_t, dtype=torch.bool)
        mask[idx] = True
        x_t = x_t * mask.float()

        # Local forgetting
        rho_next = self.rho
        if self.local_forget_eta > 0.0:
            inactive = (x_t <= 0).float().view(1, self.n)
            rho_next = rho_next * (1.0 - self.local_forget_eta * inactive)

        # Neuromodulated multi-timescale update
        inc = gain * (v_prev.view(self.d,1) @ x_t.view(1,self.n))
        if self.U_kernels is None:
            rho_next = self.u_decay * (rho_next + inc)
        else:
            wk = torch.tensor(self.U_weights if self.U_weights is not None else
                              [1.0]*len(self.U_kernels),
                              dtype=torch.float32, device=rho_next.device)
            wk = wk / wk.sum()
            rho_mix = torch.zeros_like(rho_next)
            for w, u in zip(wk, self.U_kernels):
                rho_mix += w * (u * (rho_next + inc))
            rho_next = rho_mix

        self.rho = rho_next
        self.x, self.y, self.v = x_t, y_t, v_star

        with torch.no_grad():
            spars_x = 1.0 - float((self.x != 0).float().mean().item())
            spars_y = 1.0 - float((self.y != 0).float().mean().item())
            rho_F = float(torch.linalg.norm(self.rho).item())
            rho_er = effective_rank(self.rho)
        return dict(sparsity_x=spars_x, sparsity_y=spars_y,
                    rho_F=rho_F, rho_eff_rank=rho_er, gain=gain)

[File Ends] bdh_neuro/bdh_gpu_neuro_torch.py

[File Begins] bdh_neuro/bdh_gpu_ref_stabilized.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
bdh_gpu_ref_stabilized.py — Stable BDH-GPU Reference Model
-----------------------------------------------------------
A minimal, numerically stable BDH-GPU baseline that adds:

✔ x-state decay (leaky integration)
✔ L1 normalization of x
✔ Soft thresholding for sparsity

No neuromodulation, no branches, no homeostasis — pure BDH core.
Intended for direct comparison to BDH-Neuro variant.
"""

import torch
from typing import Dict


def relu(z):
    return torch.clamp_min(z, 0.0)


def layernorm_row(z, eps: float = 1e-6):
    """Row-wise layer normalization."""
    m = z.mean(dim=-1, keepdim=True)
    s = z.std(dim=-1, keepdim=True)
    return (z - m) / (s + eps)


def effective_rank(mat: torch.Tensor, eps: float = 1e-12) -> float:
    """Effective rank based on singular-value entropy."""
    with torch.no_grad():
        s = torch.linalg.svdvals(mat)
        ps = (s**2) / (s.pow(2).sum() + eps)
        H = -(ps * (ps.add(eps).log())).sum()
        return float(torch.exp(H))


class BDHGPURefStabilized(torch.nn.Module):
    """Minimal BDH-GPU (Definition 4) with stabilizing sparsity mechanics."""

    def __init__(self, n=256, d=32, V=4096, seed=0,
                 u_decay=0.97, ln_before_Dy=True, device="cpu",
                 x_decay: float = 0.97, threshold_ratio: float = 0.02):
        """
        Args:
            n, d, V: model dimensions
            seed: RNG seed
            u_decay: forgetting factor for rho
            ln_before_Dy: layernorm position flag
            device: cpu or cuda
            x_decay: leak factor for x state
            threshold_ratio: threshold relative to x_t.max() for sparsity
        """
        super().__init__()
        g = torch.Generator(device=device).manual_seed(seed)
        self.n, self.d, self.V = n, d, V
        self.device = device
        self.u_decay = float(u_decay)
        self.ln_before_Dy = bool(ln_before_Dy)
        self.x_decay = float(x_decay)
        self.threshold_ratio = float(threshold_ratio)

        # Weight matrices (non-trainable)
        self.E  = torch.randn(d, n, generator=g, device=device) / (n**0.5)
        self.Dx = torch.randn(n, d, generator=g, device=device) / (d**0.5)
        self.Dy = torch.randn(n, d, generator=g, device=device) / (d**0.5)
        self.token_emb = torch.randn(V, d, generator=g, device=device) / (d**0.5)

        # State variables
        self.register_buffer("x", torch.zeros(n, device=device))
        self.register_buffer("y", torch.zeros(n, device=device))
        self.register_buffer("v", torch.zeros(d, device=device))
        self.register_buffer("rho", torch.zeros(d, n, device=device))
        self._rng = g

    # ------------------------------------------------------------
    # Core BDH update
    # ------------------------------------------------------------
    def step(self, token_index: int) -> Dict[str, float]:
        v_prev = self.token_emb[int(token_index)]

        # --- Stabilized x dynamics: decay, normalize, threshold ---
        x_t = self.x_decay * self.x + (self.Dx @ v_prev)
        x_t = x_t / (x_t.norm(p=1) + 1e-6)  # energy normalization

        # soft thresholding for sparsity
        threshold = self.threshold_ratio * x_t.max()
        x_t = torch.where(x_t > threshold, x_t, torch.zeros_like(x_t))

        # --- y dynamics (standard BDH flow) ---
        a_star = self.rho @ x_t
        if self.ln_before_Dy:
            y_core = self.Dy @ layernorm_row(a_star)
        else:
            y_core = layernorm_row(self.Dy @ a_star)

        y_t = relu(y_core) * torch.clamp_min(x_t, 0.0)
        v_star = layernorm_row(self.E @ y_t)

        # --- Hebbian update for rho ---
        self.rho = self.u_decay * (self.rho + v_prev.view(self.d, 1) @ x_t.view(1, self.n))

        # --- commit new state ---
        self.x, self.y, self.v = x_t, y_t, v_star

        # --- diagnostics ---
        with torch.no_grad():
            spars_x = 1.0 - float((self.x != 0).float().mean().item())
            spars_y = 1.0 - float((self.y != 0).float().mean().item())
            rho_F = float(torch.linalg.norm(self.rho).item())
            rho_er = effective_rank(self.rho)

        return dict(sparsity_x=spars_x, sparsity_y=spars_y,
                    rho_F=rho_F, rho_eff_rank=rho_er)

    # ------------------------------------------------------------
    # Run multiple tokens sequentially
    # ------------------------------------------------------------
    @torch.no_grad()
    def run(self, T: int) -> Dict[str, float]:
        out = {}
        for _ in range(T):
            idx = torch.randint(0, self.V, (1,), generator=self._rng, device=self.device).item()
            out = self.step(idx)
        return out

[File Ends] bdh_neuro/bdh_gpu_ref_stabilized.py

[File Begins] bdh_neuro/example.py
from bdh_gpu_neuro_ref import BDHGPURef, BDHNeuroRef

# Baseline (Def.4)
base = BDHGPURef(n=128, d=24, V=2048, seed=3, u_decay=0.97, ln_before_Dy=True, use_relu_lowrank=True)
print(base.run(T=32))

# Neuro-Variante mit mehreren Prinzipien
neuro = BDHNeuroRef(n=128, d=24, V=2048, seed=3,
                    U_kernels=[0.99, 0.97, 0.94], U_weights=[0.5, 0.3, 0.2],
                    local_forget_eta=0.02, homeostasis_tau=0.15*128,
                    k_wta=16, branches=2, branch_nl="softplus",
                    mod_gamma_max=0.8, spike_rate=0.01,
                    ln_before_Dy=True, use_relu_lowrank=True)
print(neuro.run(T=32))

[File Ends] bdh_neuro/example.py

[File Begins] bdh_neuro/example_cuda.py
from bdh_gpu_neuro_torch import BDHGPURefTorch, BDHNeuroRefTorch

# Baseline (Def.4)
base = BDHGPURefTorch(n=128, d=24, V=2048, seed=3, u_decay=0.97,
                      ln_before_Dy=True, use_relu_lowrank=True)
print(base.run(T=32))

# Neuro mit allen Prinzipien
neuro = BDHNeuroRefTorch(n=128, d=24, V=2048, seed=3,
                         U_kernels=[0.99, 0.97, 0.94], U_weights=[0.5, 0.3, 0.2],
                         local_forget_eta=0.02,
                         homeostasis_tau=0.15*128,
                         k_wta=16, branches=2, branch_nl='softplus',
                         mod_gamma_max=0.8, spike_rate=0.01,
                         ln_before_Dy=True, use_relu_lowrank=True,
                         device='cuda')
print(neuro.run(T=32))

[File Ends] bdh_neuro/example_cuda.py

[File Begins] bdh_neuro/generate_bdh_baseline.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_baseline.py — text generation for BDH-GPU Reference Model
-------------------------------------------------------------------
Loads the BDHGPURefTorch baseline and generates text autoregressively
from a starting prompt.

Usage:
    python generate_baseline.py --file data/divine_comedy_full.txt --start "Hello " --steps 500
"""

import argparse
import torch
from bdh_gpu_neuro_torch import BDHGPURefTorch

# ---------------------------------------------------------------
# Utility functions
# ---------------------------------------------------------------

def load_text_and_vocab(path):
    text = open(path, encoding="utf-8").read()
    vocab = sorted(list(set(text)))
    stoi = {ch: i for i, ch in enumerate(vocab)}
    itos = {i: ch for ch, i in stoi.items()}
    return text, stoi, itos


# ---------------------------------------------------------------
# Generation loop
# ---------------------------------------------------------------

def generate(args):
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    text, stoi, itos = load_text_and_vocab(args.file)
    vocab_size = len(stoi)
    print(f"Loaded vocabulary (|V|={vocab_size})")

    # Initialize model (same architecture as training)
    model = BDHGPURefTorch(n=128, d=32, V=vocab_size, seed=42, device=device).to(device)
    model.eval()

    # Optionally: load trained weights if you saved them
    # Example:
    # model.load_state_dict(torch.load("baseline_model.pt"))

    prompt = args.start
    print(f"Generating text starting with: '{prompt}'\n")

    generated = list(prompt)
    for ch in prompt:
        idx = torch.tensor([stoi.get(ch, 0)], device=device)
        _ = model.step(int(idx))  # warm-up

    # Generate continuation
    for _ in range(args.steps):
        # Linear readout from v to vocab logits
        v = model.v
        logits = v @ model.token_emb.T
        probs = torch.softmax(logits, dim=-1)
        next_idx = torch.multinomial(probs, 1).item()
        next_ch = itos[next_idx]
        generated.append(next_ch)

        # Feed back token
        _ = model.step(next_idx)

    text_out = "".join(generated)
    print("--- GENERATED TEXT ---")
    print(text_out)
    with open("generated_baseline.txt", "w", encoding="utf-8") as f:
        f.write(text_out)
    print("\nSaved to generated_baseline.txt")


# ---------------------------------------------------------------
# CLI
# ---------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", type=str, required=True, help="Path to text file (same as training)")
    parser.add_argument("--device", type=str, default="cpu", help="cpu or cuda")
    parser.add_argument("--start", type=str, default="Hello ", help="Prompt to begin generation")
    parser.add_argument("--steps", type=int, default=500, help="Number of tokens to generate")
    args = parser.parse_args()
    generate(args)

[File Ends] bdh_neuro/generate_bdh_baseline.py

[File Begins] bdh_neuro/generate_bdh_neuro.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate.py — Text generation using BDHNeuroRefTorch
----------------------------------------------------

Uses the trained model (bdh_neuro_model.pt) produced by train.py
to generate text character-by-character.

Usage:
    python generate.py --file input.txt --seed_text "Hello " --length 500 --device cuda
"""

import torch, torch.nn.functional as F
from bdh_gpu_neuro_torch import BDHNeuroRefTorch


# ------------------------------ Vocabulary utils ------------------------------
def load_vocab(path: str):
    """Load the vocabulary (character to ID mappings) from a text file."""
    text = open(path, "r", encoding="utf-8").read()
    vocab = sorted(set(text))
    stoi = {ch: i for i, ch in enumerate(vocab)}
    itos = {i: ch for ch, i in stoi.items()}
    return stoi, itos


# ------------------------------ Generation logic ------------------------------
def generate_text(args):
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    stoi, itos = load_vocab(args.file)
    V = len(stoi)
    print(f"Loaded vocabulary (|V|={V})")

    # Rebuild model with identical parameters as training
    model = BDHNeuroRefTorch(
        n=args.n, d=args.d, V=V, seed=3, device=device,
        U_kernels=[0.99, 0.97, 0.94], U_weights=[0.5, 0.3, 0.2],
        local_forget_eta=0.02,
        homeostasis_tau=0.15 * args.n,
        k_wta=max(1, args.n // 8),
        branches=2, branch_nl="softplus",
        mod_gamma_max=0.8, spike_rate=0.01,
        ln_before_Dy=True, use_relu_lowrank=True
    ).to(device)

    # Prediction head (same as in train.py)
    head = torch.nn.Linear(model.d, V).to(device)

    # Load saved weights
    state = torch.load("bdh_neuro_model.pt", map_location=device)
    model.load_state_dict({k.replace("core.", ""): v for k, v in state.items() if k.startswith("core.")}, strict=False)
    head.load_state_dict({k.replace("head.", ""): v for k, v in state.items() if k.startswith("head.")}, strict=False)
    model.eval(); head.eval()

    # Reset internal states
    model.x.zero_(); model.y.zero_(); model.v.zero_(); model.rho.zero_()

    # Initialize with seed text
    seed_text = args.seed_text
    for ch in seed_text[:-1]:
        idx = torch.tensor([stoi.get(ch, 0)], device=device)
        model.step(int(idx))

    current_char = seed_text[-1]
    output = seed_text

    print(f"Generating text starting with: '{seed_text}'")
    for t in range(args.length):
        idx = torch.tensor([stoi.get(current_char, 0)], device=device)
        model.step(int(idx))
        logits = head(model.v).view(1, -1)
        probs = F.softmax(logits / args.temperature, dim=-1)
        next_id = torch.multinomial(probs, num_samples=1).item()
        next_char = itos[next_id]
        output += next_char
        current_char = next_char

    print("\n--- GENERATED TEXT ---")
    print(output)
    with open("generated.txt", "w", encoding="utf-8") as f:
        f.write(output)
    print("\nSaved to generated.txt")


# ------------------------------ CLI entry point ------------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", type=str, default="input.txt", help="Training text file (for vocab reconstruction)")
    parser.add_argument("--seed_text", type=str, default="Hello ", help="Initial text to start generation")
    parser.add_argument("--length", type=int, default=500, help="Number of characters to generate")
    parser.add_argument("--temperature", type=float, default=0.8, help="Sampling temperature (lower = more deterministic)")
    parser.add_argument("--n", type=int, default=128)
    parser.add_argument("--d", type=int, default=32)
    parser.add_argument("--device", type=str, default="cpu")
    args = parser.parse_args()
    generate_text(args)

[File Ends] bdh_neuro/generate_bdh_neuro.py

[File Begins] bdh_neuro/plot_sparsity.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
plot_sparsity.py — visualize BDH-Neuro dynamics
-----------------------------------------------
Reads training logs printed by train.py (from stdout or a saved log file)
and plots sx, sy, ρF, gain, and loss over time.

Usage:
    python plot_sparsity.py --file train.log
"""

import re, argparse
import matplotlib.pyplot as plt

pattern = re.compile(
    r"loss=([\d\.]+)\s+sx=([\d\.]+)\s+sy=([\d\.]+)\s+ρF=([\d\.]+)\s+gain=([\d\.]+)"
)

def parse_log(file):
    losses, sx, sy, rhoF, gain = [], [], [], [], []
    for line in open(file, "r", encoding="utf-8"):
        m = pattern.search(line)
        if m:
            l, x, y, r, g = map(float, m.groups())
            losses.append(l); sx.append(x); sy.append(y); rhoF.append(r); gain.append(g)
    return losses, sx, sy, rhoF, gain

def plot_metrics(losses, sx, sy, rhoF, gain):
    fig, axs = plt.subplots(5, 1, figsize=(8, 12), sharex=True)
    axs[0].plot(losses); axs[0].set_ylabel("Loss")
    axs[1].plot(sx); axs[1].set_ylabel("Sparsity x")
    axs[2].plot(sy); axs[2].set_ylabel("Sparsity y")
    axs[3].plot(rhoF); axs[3].set_ylabel("ρF (norm)")
    axs[4].plot(gain); axs[4].set_ylabel("Gain")
    axs[-1].set_xlabel("Step (per 500 updates)")
    for ax in axs: ax.grid(True, alpha=0.3)
    plt.tight_layout(); plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", type=str, default="train.log",
                        help="Path to log file containing training prints")
    args = parser.parse_args()
    metrics = parse_log(args.file)
    plot_metrics(*metrics)

[File Ends] bdh_neuro/plot_sparsity.py

[File Begins] bdh_neuro/train_bdh_baseline.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_bdh_baseline.py — Baseline (self-organizing) BDH-GPU Reference
---------------------------------------------------------------------
Evaluates the purely dynamical BDHGPURefTorch model on a text corpus.

No gradient descent is used — this baseline tests intrinsic BDH dynamics.
Outputs diagnostics every N steps.

Usage:
    python train_bdh_baseline.py --file data/divine_comedy_full.txt --device cuda
"""

import argparse
import torch
import torch.nn.functional as F
from bdh_gpu_ref_stabilized import BDHGPURefTorch


# ---------------------------------------------------------------
# Utility: load text and create vocabulary
# ---------------------------------------------------------------

def load_text_and_vocab(path):
    text = open(path, encoding="utf-8").read()
    vocab = sorted(list(set(text)))
    stoi = {ch: i for i, ch in enumerate(vocab)}
    itos = {i: ch for ch, i in stoi.items()}
    return text, stoi, itos


# ---------------------------------------------------------------
# Training (self-organizing evaluation) loop
# ---------------------------------------------------------------

def train(args):
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    text, stoi, itos = load_text_and_vocab(args.file)
    print(f"Loaded text of length {len(text)} with vocab size {len(stoi)}")

    # Hyperparameters (same as other experiments)
    n, d = 128, 32
    seq_len = 64
    batch_size = 64

    # Initialize BDH-GPU reference model
    model = BDHGPURefTorch(n=n, d=d, V=len(stoi), seed=42, device=device).to(device)
    model.eval()  # no learnable parameters, so no training mode needed

    # Batch sampling utility
    def sample_batch():
        ix = torch.randint(0, len(text) - seq_len - 1, (batch_size,))
        x = torch.tensor([[stoi[c] for c in text[i:i+seq_len]] for i in ix], device=device)
        y = torch.tensor([[stoi[c] for c in text[i+1:i+seq_len+1]] for i in ix], device=device)
        return x, y

    # Main loop
    for step in range(1, args.steps + 1):
        x, y = sample_batch()
        logits = []

        # Feed the sequence token by token
        for t in range(seq_len):
            metrics = model.step(int(x[0, t]))  # one token per step
            v = model.v
            logit = v @ model.token_emb.T
            logits.append(logit.unsqueeze(0))

        # Diagnostic cross-entropy loss (not backpropagated)
        logits = torch.cat(logits, dim=0)  # (seq_len, vocab)
        targets = y[0]
        loss = F.cross_entropy(logits, targets)

        # Logging
        if step % args.log_interval == 0:
            print(
                f"[step {step}] loss={loss.item():.4f} "
                f"sx={metrics['sparsity_x']:.2f} sy={metrics['sparsity_y']:.2f} "
                f"ρF={metrics['rho_F']:.2f} rank={metrics['rho_eff_rank']:.2f}"
            )

    print("BDH baseline dynamics evaluation complete.")


# ---------------------------------------------------------------
# CLI
# ---------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", type=str, required=True, help="Path to text file for training")
    parser.add_argument("--device", type=str, default="cpu", help="cpu or cuda")
    parser.add_argument("--steps", type=int, default=20000, help="Total number of iterations")
    parser.add_argument("--log_interval", type=int, default=500, help="Steps between logs")
    args = parser.parse_args()
    train(args)

[File Ends] bdh_neuro/train_bdh_baseline.py

[File Begins] bdh_neuro/train_bdh_neuro.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train.py — Train the BDHNeuroRefTorch model on a text corpus
------------------------------------------------------------

This script trains the biologically-inspired BDH-GPU architecture (Definition 4 + all
neuro extensions) on a given text file (e.g., input.txt). The model learns
next-character prediction (a simple language modeling task).

All neuro-inspired options are active:
- Multi-timescale STDP-like decay (U_kernels)
- Local synaptic forgetting
- Homeostatic activity target
- k-WTA lateral inhibition
- Dendritic subunits (branch nonlinearities)
- Neuromodulation via surprisal
- Stochastic spikes

Usage:
    python train.py --file input.txt --epochs 5 --device cuda
"""

import argparse, torch, torch.nn.functional as F
from torch import nn, optim
from bdh_gpu_neuro_torch import BDHNeuroRefTorch


# ------------------------------ Simple text tokenizer ------------------------------
def load_text_as_ids(path: str):
    """Reads a text file and maps each unique character to an integer ID."""
    text = open(path, "r", encoding="utf-8").read()
    vocab = sorted(set(text))
    stoi = {ch: i for i, ch in enumerate(vocab)}
    ids = torch.tensor([stoi[ch] for ch in text], dtype=torch.long)
    return ids, stoi, {i: ch for ch, i in stoi.items()}


# ------------------------------ Wrapper model ------------------------------
class BDHLanguageModel(nn.Module):
    """A simple wrapper that predicts next-token logits from the BDHNeuroRefTorch core."""
    def __init__(self, model: BDHNeuroRefTorch):
        super().__init__()
        self.core = model
        self.head = nn.Linear(model.d, model.V)  # maps v* to next-token logits

    def forward(self, token_idx):
        """Processes one token and returns logits + internal metrics."""
        metrics = self.core.step(int(token_idx))
        logits = self.head(self.core.v)
        return logits, metrics


# ------------------------------ Training loop ------------------------------
def train(args):
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    ids, stoi, itos = load_text_as_ids(args.file)
    print(f"Loaded text of length {len(ids)} with vocab size {len(stoi)}")

    # Build model with ALL neuro options enabled
    model = BDHNeuroRefTorch(
        n=args.n, d=args.d, V=len(stoi), seed=3, device=device,
        U_kernels=[0.99, 0.97, 0.94], U_weights=[0.5, 0.3, 0.2],
        local_forget_eta=0.02,
        homeostasis_tau=0.15 * args.n,
        k_wta=max(1, args.n // 8),
        branches=2, branch_nl="softplus",
        mod_gamma_max=0.8, spike_rate=0.01,
        ln_before_Dy=True, use_relu_lowrank=True,
        x_decay=0.97
    ).to(device)

    lm = BDHLanguageModel(model).to(device)
    opt = optim.AdamW(lm.parameters(), lr=args.lr)
    loss_fn = nn.CrossEntropyLoss()

    ids = ids.to(device)

    for epoch in range(1, args.epochs + 1):
        total_loss = 0.0
        # Reset internal states before each epoch
        lm.core.x.zero_(); lm.core.y.zero_(); lm.core.v.zero_(); lm.core.rho.zero_()

        for t in range(len(ids) - 1):
            x_idx, y_idx = ids[t], ids[t + 1]
            logits, metrics = lm(x_idx)
            loss = loss_fn(logits.view(1, -1), y_idx.view(1))
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item()

            if (t + 1) % 500 == 0:
                print(f"[epoch {epoch}] step {t+1}/{len(ids)} "
                      f"loss={loss.item():.4f} "
                      f"sx={metrics['sparsity_x']:.2f} sy={metrics['sparsity_y']:.2f} "
                      f"ρF={metrics['rho_F']:.2f} gain={metrics['gain']:.2f}")

        print(f"Epoch {epoch}: mean loss = {total_loss / len(ids):.5f}")

    # Save trained model
    torch.save(lm.state_dict(), "bdh_neuro_model.pt")
    print("Model saved to bdh_neuro_model.pt")


# ------------------------------ CLI entry point ------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", type=str, default="input.txt", help="Path to training text file")
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--n", type=int, default=128, help="Neuron dimension")
    parser.add_argument("--d", type=int, default=32, help="Latent (embedding) dimension")
    parser.add_argument("--device", type=str, default="cpu", help="cuda or cpu")
    args = parser.parse_args()
    train(args)

[File Ends] bdh_neuro/train_bdh_neuro.py

[File Begins] bdh_neuro/train_bdh_refstyle.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_bdh_refstyle.py — Reference-style trainer for stabilized BDH baseline
----------------------------------------------------------------------------
- Byte-level dataset via memmap (same as your reference train.py)
- Autocast/GradScaler for mixed precision
- AdamW optimizer
- Trains ONLY a small readout head on top of the nonparametric BDH core
  (core dynamics: self-organizing; readout: supervised)
"""

import os
from contextlib import nullcontext
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Import the stabilized baseline (from the file you saved earlier)
from bdh_gpu_ref_stabilized import BDHGPURefStabilized

# ----------------------- CLI -----------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--file", type=str, default="input.txt", help="path to byte text file")
    p.add_argument("--block_size", type=int, default=512)
    p.add_argument("--batch_size", type=int, default=32)
    p.add_argument("--iters", type=int, default=3000)
    p.add_argument("--lr", type=float, default=1e-3)
    p.add_argument("--wd", type=float, default=0.1)
    p.add_argument("--log_freq", type=int, default=100)
    p.add_argument("--device", type=str, default="cuda")
    p.add_argument("--n", type=int, default=128, help="BDH n (neurons)")
    p.add_argument("--d", type=int, default=32, help="BDH latent dim")
    p.add_argument("--seed", type=int, default=1337)
    p.add_argument("--dtype", type=str, default="float16", choices=["float32","bfloat16","float16"])
    p.add_argument("--x_decay", type=float, default=0.97)
    p.add_argument("--threshold_ratio", type=float, default=0.02)
    return p.parse_args()

# ----------------------- Data -----------------------
def get_batch(memmap_bytes, block_size, batch_size, device):
    ix = torch.randint(len(memmap_bytes) - block_size - 1, (batch_size,))
    x = torch.stack([torch.from_numpy(memmap_bytes[i:i+block_size].astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy(memmap_bytes[i+1:i+1+block_size].astype(np.int64)) for i in ix])
    if device.type == "cuda":
        x = x.pin_memory().to(device, non_blocking=True)
        y = y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x, y

# ----------------------- Model wrapper -----------------------
class BDHReadout(nn.Module):
    """
    Wrap BDHGPURefStabilized core with a trainable linear head only.
    Core remains nonparametric (no nn.Parameters).
    """
    def __init__(self, core: BDHGPURefStabilized, vocab_size: int):
        super().__init__()
        self.core = core.eval()  # core is self-organizing; no gradients
        self.lm_head = nn.Linear(core.d, vocab_size)  # trainable readout

    def forward_seq(self, ids: torch.Tensor):
        """
        ids: (T,) int64 sequence for one stream.
        Returns logits: (T, vocab), and last-step metrics from core.
        """
        T = ids.size(0)
        logits = []
        metrics = {}
        for t in range(T):
            metrics = self.core.step(int(ids[t]))
            v = self.core.v  # (d,)
            logits.append(self.lm_head(v).unsqueeze(0))
        return torch.cat(logits, dim=0), metrics

# ----------------------- Train -----------------------
def main():
    args = parse_args()
    torch.manual_seed(args.seed)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # dtype/AMP context
    ptdtype = {"float32": torch.float32, "bfloat16": torch.bfloat16, "float16": torch.float16}[args.dtype]
    ctx = (torch.amp.autocast(device_type=device.type, dtype=ptdtype)
           if device.type == "cuda" else nullcontext())
    scaler = torch.amp.GradScaler(device=device.type, enabled=(args.dtype == "float16"))

    # load bytes as memmap (like the reference)
    if not os.path.exists(args.file):
        raise FileNotFoundError(f"Data file not found: {args.file}")
    data_mm = np.memmap(args.file, dtype=np.uint8, mode="r")
    vocab_size = 256  # byte-level

    # core (nonparametric) + readout head
    core = BDHGPURefStabilized(n=args.n, d=args.d, V=vocab_size,
                               seed=args.seed, device=str(device),
                               x_decay=args.x_decay, threshold_ratio=args.threshold_ratio)
    model = BDHReadout(core, vocab_size=vocab_size).to(device)
    model = torch.compile(model) if hasattr(torch, "compile") else model

    # optimize head params only
    optimizer = torch.optim.AdamW(model.lm_head.parameters(), lr=args.lr, weight_decay=args.wd)

    loss_acc = 0.0
    loss_steps = 0

    for step in range(1, args.iters + 1):
        x, y = get_batch(data_mm, args.block_size, args.batch_size, device)

        # For simplicity and to stay faithful to BDH core being single-stream stateful,
        # train on the first sequence in the batch (you can loop over batch if desired).
        seq = x[0]    # (T,)
        tgt = y[0]    # (T,)

        # reset core state each iteration (so streams don't leak)
        with torch.no_grad():
            core.x.zero_(); core.y.zero_(); core.v.zero_(); core.rho.zero_()

        with ctx:
            logits, metrics = model.forward_seq(seq)  # (T, vocab)
            loss = F.cross_entropy(logits, tgt)

        loss_acc += float(loss.item()); loss_steps += 1
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)

        if step % args.log_freq == 0:
            print(f"[{step}/{args.iters}] loss={loss_acc/loss_steps:.3f} "
                  f"sx={metrics['sparsity_x']:.2f} sy={metrics['sparsity_y']:.2f} "
                  f"ρF={metrics['rho_F']:.2f} rank={metrics['rho_eff_rank']:.2f}")
            loss_acc = 0.0; loss_steps = 0

    # Save head weights
    torch.save(model.state_dict(), "bdh_refstyle_head.pt")
    print("Done. Saved readout to bdh_refstyle_head.pt")

if __name__ == "__main__":
    main()

[File Ends] bdh_neuro/train_bdh_refstyle.py

[File Begins] bdh_original/LICENSE.md
Copyright 2025 Pathway Technology, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

[File Ends] bdh_original/LICENSE.md

[File Begins] bdh_original/README.md
# Baby Dragon Hatchling

## **Bridging the Gap Between Transformers and the Brain**

**Baby Dragon Hatchling (BDH)** is a biologically inspired large language model architecture that connects principles of deep learning with the foundations of neuroscience. Developed by researchers at [Pathway](https://pathway.com), BDH provides a theoretical and practical framework for understanding the emergence of reasoning and generalization in artificial systems.

This repository contains the official implementation from the paper:
> *A. Kosowski, P. Uznański, J. Chorowski, Z. Stamirowska, M. Bartoszkiewicz.*
> [_The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain_](https://doi.org/10.48550/arXiv.2509.26507), arXiv (2025).


## Overview

BDH represents a **scale-free, locally interacting network of neurons** capable of intrinsic reasoning dynamics. BDH scales like a Transformer on performance benchmarks—yet retains full interpretability and theoretical grounding in the fine-grained dynamics of neuron interactions.

**Key properties:**

- **Scale-free network topology** mimicking biological connectivity
- **Locally interacting neuron particles** with excitatory/inhibitory dynamics
- **Hebbian working memory** based on synaptic plasticity, displaying monosemanticity
- **GPU-friendly state-space formulation** for efficient implementation
- **Interpretable activations** that are sparse and positive

BDH formalizes a bridge between **neural computation and machine-based language understanding**. It shows how **macro reasoning behavior** in large AI models emerges from **micro-level neuron dynamics**, guided by principles of graph theory and local computation.

Empirically, BDH matches **GPT-2–scale Transformers** across language and translation tasks at equivalent parameter scales (10M–1B).


***

## Architecture

<img src="figs/architecture.png" width="600"/>

***

## Relation to Transformers

<img src="figs/vocab.png" width="600"/>

BDH and the Transformer share attention-inspired computation; however, BDH’s graph-based architecture makes its attention **emerge naturally from neuron-level interactions**, reflecting attention as seen in biological systems.

***

## Scaling Laws

<img src="figs/bdh_scaling.png" width="600"/>

BDH follows **Transformer-like scaling laws**, maintaining parameter efficiency while achieving interpretability at any scale.

***

## Installation and Training

```bash
# install dependencies
pip install -r requirements.txt

# train BDH on a toy dataset
python train.py
```

<!--For visualization and interpretability analysis, explore the example notebooks in `notebooks/`.-->



## Learn and Discuss

- Watch the *SuperDataScience podcast* [▶️ *Dragon Hatchling: The Missing Link Between Transformers and the Brain*](https://www.youtube.com/watch?v=mfV44-mtg7c) (72 min.) featuring Adrian Kosowski in conversation with Jon Krohn, unpacking BDH’s neuron-level architecture and sparse reasoning dynamics.

- Read about BDH in
[*Forbes*](https://www.forbes.com/sites/victordey/2025/10/08/can-ai-learn-and-evolve-like-a-brain-pathways-bold-research-thinks-so/),
[*Semafor*](https://www.semafor.com/article/10/01/2025/new-ai-research-claims-to-be-getting-closer-to-modeling-human-brain),
[*The Turing Post*](https://www.turingpost.com/p/fod-121-300-million-to-start-a-big-promise-for-science#the-freshest-research-papers-catego),
[*Quantum Zeitgeist*](https://quantumzeitgeist.com/palo-alto-ai-firm-pathway-unveils-post-transformer-architecture-for-autonomous-ai/),
[*Golem*](https://www.golem.de/news/neue-ki-architektur-was-ist-baby-dragon-hatchling-2510-201047-2.html),
and elsewhere in the media.

- Discuss and share the BDH paper on:
[*Hugging Face Papers*](https://huggingface.co/papers/2509.26507), 
[*Alphaxiv*](https://alphaxiv.org/abs/2509.26507),
and [*EmergentMind*](https://emergentmind.com/papers/2509.26507).

## Community Forks

- [adamskrodzki/bdh](https://github.com/adamskrodzki/bdh): dynamic vocabulary, stateful attention
- [mosure/burn_dragon_hatchling](https://github.com/mosure/burn_dragon_hatchling): Burn port
- [severian42/bdh](https://github.com/severian42/bdh): MLX port
- [Git-Faisal/bdh](https://github.com/Git-Faisal/bdh)
- [GrahLnn/bdh](https://github.com/GrahLnn/bdh)

## Acknowledgements
We thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.

BDH research stands at the intersection of **AI architecture**, **biological learning models**, and **theoretical computer science**—an effort to map the *equations of reasoning* between artificial and biological intelligence.

[File Ends] bdh_original/README.md

[File Begins] bdh_original/bdh.py
# Copyright 2025 Pathway Technology, Inc.

import dataclasses
import math

import torch
import torch.nn.functional as F
from torch import nn


@dataclasses.dataclass
class BDHConfig:
    n_layer: int = 6
    n_embd: int = 256
    dropout: float = 0.1
    n_head: int = 4
    mlp_internal_dim_multiplier: int = 128
    vocab_size: int = 256


def get_freqs(n, theta, dtype):
    def quantize(t, q=2):
        return (t / q).floor() * q

    return (
        1.0
        / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))
        / (2 * math.pi)
    )


class Attention(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        nh = config.n_head
        D = config.n_embd
        N = config.mlp_internal_dim_multiplier * D // nh
        self.freqs = torch.nn.Buffer(
            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)
        )

    @staticmethod
    def phases_cos_sin(phases):
        phases = (phases % 1) * (2 * math.pi)
        phases_cos = torch.cos(phases)
        phases_sin = torch.sin(phases)
        return phases_cos, phases_sin

    @staticmethod
    def rope(phases, v):
        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())
        phases_cos, phases_sin = Attention.phases_cos_sin(phases)
        return (v * phases_cos).to(v.dtype) + (v_rot * phases_sin).to(v.dtype)

    def forward(self, Q, K, V):
        assert self.freqs.dtype == torch.float32
        assert K is Q
        _, _, T, _ = Q.size()

        r_phases = (
            torch.arange(
                0,
                T,
                device=self.freqs.device,
                dtype=self.freqs.dtype,
            ).view(1, 1, -1, 1)
        ) * self.freqs
        QR = self.rope(r_phases, Q)
        KR = QR

        # Current attention
        scores = (QR @ KR.mT).tril(diagonal=-1)
        return scores @ V


class BDH(nn.Module):
    def __init__(self, config: BDHConfig):
        super().__init__()
        assert config.vocab_size is not None
        self.config = config
        nh = config.n_head
        D = config.n_embd
        N = config.mlp_internal_dim_multiplier * D // nh
        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))
        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))

        self.attn = Attention(config)

        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)
        self.embed = nn.Embedding(config.vocab_size, D)
        self.drop = nn.Dropout(config.dropout)
        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))

        self.lm_head = nn.Parameter(
            torch.zeros((D, config.vocab_size)).normal_(std=0.02)
        )

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        C = self.config

        B, T = idx.size()
        D = C.n_embd
        nh = C.n_head
        N = D * C.mlp_internal_dim_multiplier // nh

        x = self.embed(idx).unsqueeze(1)

        # actually helps with training
        x = self.ln(x)  # B, 1, T, D

        for level in range(C.n_layer):
            x_latent = x @ self.encoder

            x_sparse = F.relu(x_latent)  # B, nh, T, N

            yKV = self.attn(
                Q=x_sparse,
                K=x_sparse,
                V=x,
            )
            yKV = self.ln(yKV)

            y_latent = yKV @ self.encoder_v
            y_sparse = F.relu(y_latent)
            xy_sparse = x_sparse * y_sparse  # B, nh, T, N

            xy_sparse = self.drop(xy_sparse)

            yMLP = (
                xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder
            )  # B, 1, T, D
            y = self.ln(yMLP)
            x = self.ln(x + y)

        logits = x.view(B, T, D) @ self.lm_head
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))

        return logits, loss

    @torch.no_grad()
    def generate(
        self,
        idx: torch.Tensor,
        max_new_tokens: int,
        temperature: float = 1.0,
        top_k: int | None = None,
    ) -> torch.Tensor:
        for _ in range(max_new_tokens):
            idx_cond = idx
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / temperature
            if top_k is not None:
                values, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < values[:, [-1]]] = float("-inf")
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

[File Ends] bdh_original/bdh.py

[File Begins] bdh_original/train.py
# Copyright Pathway Technology, Inc.

import os
from contextlib import nullcontext

import bdh
import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# On a Mac you can also try
# device=torch.device('mps')

dtype = (
    "bfloat16"
    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    else "float16"
)  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
ptdtype = {
    "float32": torch.float32,
    "bfloat16": torch.bfloat16,
    "float16": torch.float16,
}[dtype]
ctx = (
    torch.amp.autocast(device_type=device.type, dtype=ptdtype)
    if "cuda" in device.type
    else nullcontext()
)
scaler = torch.amp.GradScaler(device=device.type, enabled=(dtype == "float16"))
torch.manual_seed(1337)
torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
print(f"Using device: {device} with dtype {dtype}")


# Configuration
BDH_CONFIG = bdh.BDHConfig()
BLOCK_SIZE = 512
BATCH_SIZE = 4
MAX_ITERS = 3000
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 0.1
LOG_FREQ = 100

input_file_path = os.path.join(os.path.dirname(__file__), "input.txt")


# Fetch the tiny Shakespeare dataset
def fetch_data():
    if not os.path.exists(input_file_path):
        data_url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        with open(input_file_path, "w") as f:
            f.write(requests.get(data_url).text)


def get_batch(split):
    # treat the file as bytes
    data = np.memmap(input_file_path, dtype=np.uint8, mode="r")
    if split == "train":
        data = data[: int(0.9 * len(data))]
    else:
        data = data[int(0.9 * len(data)) :]
    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))
    x = torch.stack(
        [torch.from_numpy((data[i : i + BLOCK_SIZE]).astype(np.int64)) for i in ix]
    )
    y = torch.stack(
        [
            torch.from_numpy((data[i + 1 : i + 1 + BLOCK_SIZE]).astype(np.int64))
            for i in ix
        ]
    )
    if torch.cuda.is_available():
        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(
            device, non_blocking=True
        )
    else:
        x, y = x.to(device), y.to(device)
    return x, y


def eval(model):
    model.eval()


if __name__ == "__main__":
    fetch_data()

    model = bdh.BDH(BDH_CONFIG).to(device)
    model = torch.compile(model)
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
    )

    x, y = get_batch("train")

    loss_acc = 0
    loss_steps = 0
    for step in range(MAX_ITERS):
        with ctx:
            logits, loss = model(x, y)
        x, y = get_batch("train")
        loss_acc += loss
        loss_steps += 1
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
        if step % LOG_FREQ == 0:
            print(f"Step: {step}/{MAX_ITERS} loss {loss_acc.item() / loss_steps:.3}")
            loss_acc = 0
            loss_steps = 0
    print("Training done, now generating a sample ")
    model.eval()
    prompt = torch.tensor(
        bytearray("To be or ", "utf-8"), dtype=torch.long, device=device
    ).unsqueeze(0)
    ret = model.generate(prompt, max_new_tokens=100, top_k=3)
    ret_decoded = bytes(ret.to(torch.uint8).to("cpu").squeeze(0)).decode(
        errors="backslashreplace"
    )
    print(ret_decoded)

[File Ends] bdh_original/train.py


<-- File Content Ends

